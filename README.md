# AI Scaling Bottlenecks Simulator

The simulator confirms that while the Scaling Laws remain mathematically valid, the Physical Substrate is nearing a phase transition. Future gains will likely derive from architectural ingenuity rather than sheer capital expenditure.

## Overview

The **AI Scaling Bottlenecks Simulator** models large AI model training under multiple constraints, including:

- Compute capacity
- Memory bandwidth
- Budget and cost
- Energy and thermal limits
- Data availability
- Network/interconnect efficiency

It is designed to explore how these factors interact over time, helping researchers, engineers, and students visualize potential bottlenecks in scaling AI systems.

**Inspired by:**

- Kaplan et al., *"Scaling Laws for Neural Language Models"* (2020)  
- Hoffmann et al., *"Chinchilla: Compute-Optimal LLMs"* (2022)  
- MLPerf HPC benchmarks for network and interconnect efficiency  

---

## Key Features

- Multi-dimensional bottleneck simulation for AI training workloads  
- Parametric modeling of hardware growth, cost, energy, and network efficiency  
- Conceptual implementation of Chinchilla-style scaling laws  
- ASCII phase diagrams and timeline tables for visual analysis  
- Fully configurable for sensitivity analysis over multiple years  

---

## Disclaimer

This simulator implements **known AI scaling laws heuristically** to model compute, memory, data, cost, thermal, verification, and interconnect bottlenecks.  

While grounded in established scaling research, it has **not been empirically validated** against real-world large-scale training data.  

Therefore, the framework provides **conceptual and predictive insights** for exploratory analysis rather than confirmed empirical results. Users should treat all outputs as **illustrative and heuristic**, not definitive.  


